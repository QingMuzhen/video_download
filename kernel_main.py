"""è§†é¢‘çˆ¬è™«å·¥å…· - ä¸»ç¨‹åºï¼ˆæ™ºèƒ½è‡ªåŠ¨æ¨¡å¼ï¼‰"""

import argparse
import sys
import os
import requests
from utils import (
    setup_logger,
    VideoParser,
    VideoDownloader,
    NetworkCapture,
    MediaMerger,
    StreamDownloader,
    SmartDetector,
    EncryptedVideoHandler
)


def get_html_content(url, proxy=None, logger=None):
    """
    è·å–ç½‘é¡µHTMLå†…å®¹
    
    Args:
        url: ç›®æ ‡URL
        proxy: ä»£ç†æœåŠ¡å™¨åœ°å€
        logger: æ—¥å¿—è®°å½•å™¨
    
    Returns:
        str: HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        proxies = {'http': proxy, 'https': proxy} if proxy else None
        
        if logger:
            logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")
        
        response = requests.get(url, headers=headers, timeout=30, proxies=proxies)
        response.raise_for_status()
        
        if logger:
            logger.info(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œå¤§å°: {len(response.text)} å­—èŠ‚")
        
        return response.text
    
    except requests.exceptions.RequestException as e:
        if logger:
            logger.error(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
        else:
            print(f"è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
        return None
    except Exception as e:
        if logger:
            logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
        else:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
        return None


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    missing_deps = []
    
    try:
        import requests
    except ImportError:
        missing_deps.append('requests')
    
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        missing_deps.append('beautifulsoup4')
    
    try:
        import tqdm
    except ImportError:
        missing_deps.append('tqdm')
    
    if missing_deps:
        print("é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“")
        print(f"ç¼ºå¤±ä¾èµ–: {', '.join(missing_deps)}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
        print("  pip install -r requirements.txt")
        print("\næˆ–è€…æ‰‹åŠ¨å®‰è£…:")
        print(f"  pip install {' '.join(missing_deps)}")
        return False
    
    return True


def check_optional_dependencies():
    """æ£€æŸ¥å¯é€‰ä¾èµ–"""
    optional_deps = {}
    
    try:
        import selenium
        optional_deps['selenium'] = True
    except ImportError:
        optional_deps['selenium'] = False
    
    return optional_deps


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è§†é¢‘çˆ¬è™«å·¥å…· - æ™ºèƒ½è‡ªåŠ¨æ£€æµ‹æ¨¡å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è‡ªåŠ¨æ¨¡å¼ï¼ˆæ¨èï¼‰- ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©æœ€ä½³ç­–ç•¥
  %(prog)s https://example.com/videos
  
  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œä¸‹è½½æ•°é‡
  %(prog)s https://example.com/videos -o ./my_videos -m 20
  
  # ä½¿ç”¨ä»£ç†
  %(prog)s https://example.com/videos --proxy http://127.0.0.1:7890
  
  # æ‰‹åŠ¨æŒ‡å®šæ¨¡å¼ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰
  %(prog)s https://example.com/videos --force-capture
  %(prog)s https://example.com/playlist.m3u8 --force-hls
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('url', help='ç›®æ ‡ç½‘ç«™URL')
    
    # åŸºæœ¬é€‰é¡¹
    basic_group = parser.add_argument_group('åŸºæœ¬é€‰é¡¹')
    basic_group.add_argument(
        '-o', '--output',
        default='downloads',
        help='è§†é¢‘ä¿å­˜ç›®å½• (é»˜è®¤: downloads)'
    )
    basic_group.add_argument(
        '-m', '--max-downloads',
        type=int,
        default=10,
        help='æœ€å¤§ä¸‹è½½æ•°é‡ (é»˜è®¤: 10)'
    )
    
    # ä¸‹è½½é€‰é¡¹
    download_group = parser.add_argument_group('ä¸‹è½½é€‰é¡¹')
    download_group.add_argument(
        '-w', '--workers',
        type=int,
        default=3,
        help='å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 3)'
    )
    download_group.add_argument(
        '-r', '--retries',
        type=int,
        default=3,
        help='ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)'
    )
    download_group.add_argument(
        '--proxy',
        help='ä»£ç†æœåŠ¡å™¨åœ°å€ (ä¾‹å¦‚: http://127.0.0.1:7890)'
    )
    download_group.add_argument(
        '--resume',
        action='store_true',
        help='å¯ç”¨æ–­ç‚¹ç»­ä¼ '
    )
    download_group.add_argument(
        '--no-verify',
        action='store_true',
        help='è·³è¿‡æ–‡ä»¶å®Œæ•´æ€§éªŒè¯'
    )
    
    # é«˜çº§é€‰é¡¹ï¼ˆæ‰‹åŠ¨æ§åˆ¶ï¼‰
    advanced_group = parser.add_argument_group('é«˜çº§é€‰é¡¹ï¼ˆè¦†ç›–è‡ªåŠ¨æ£€æµ‹ï¼‰')
    advanced_group.add_argument(
        '--force-capture',
        action='store_true',
        help='å¼ºåˆ¶ä½¿ç”¨æŠ“åŒ…æ¨¡å¼'
    )
    advanced_group.add_argument(
        '--force-hls',
        action='store_true',
        help='å¼ºåˆ¶ä½¿ç”¨HLSä¸‹è½½æ¨¡å¼'
    )
    advanced_group.add_argument(
        '--no-merge',
        action='store_true',
        help='ç¦ç”¨è‡ªåŠ¨éŸ³è§†é¢‘åˆå¹¶'
    )
    advanced_group.add_argument(
        '--wait-time',
        type=int,
        default=10,
        help='æŠ“åŒ…æ¨¡å¼é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ (é»˜è®¤: 10)'
    )
    advanced_group.add_argument(
        '--keywords',
        help='æœç´¢å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”å¤šä¸ªå…³é”®è¯ï¼Œä¾‹å¦‚: video,stream,playï¼‰'
    )
    
    # å…¶ä»–é€‰é¡¹
    other_group = parser.add_argument_group('å…¶ä»–é€‰é¡¹')
    other_group.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)'
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # æ£€æŸ¥å¯é€‰ä¾èµ–
    optional_deps = check_optional_dependencies()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # è®¾ç½®æ—¥å¿—
    import logging
    log_level = getattr(logging, args.log_level)
    logger = setup_logger(level=log_level)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=" * 70)
    print("è§†é¢‘çˆ¬è™«å·¥å…· v3.0 - æ™ºèƒ½è‡ªåŠ¨æ£€æµ‹æ¨¡å¼")
    print("=" * 70)
    print(f"ç›®æ ‡URL: {args.url}")
    print(f"ä¿å­˜ç›®å½•: {args.output}")
    print(f"æœ€å¤§ä¸‹è½½æ•°: {args.max_downloads}")
    print(f"å¹¶å‘çº¿ç¨‹æ•°: {args.workers}")
    if args.proxy:
        print(f"ä»£ç†æœåŠ¡å™¨: {args.proxy}")
    print("=" * 70)
    print()
    
    logger.info("ç¨‹åºå¯åŠ¨ - æ™ºèƒ½è‡ªåŠ¨æ£€æµ‹æ¨¡å¼")
    
    try:
        # åˆ›å»ºæ™ºèƒ½æ£€æµ‹å™¨
        detector = SmartDetector(logger=logger)
        
        # åˆæ­¥æ£€æµ‹URLç±»å‹
        print("ğŸ” æ­£åœ¨åˆ†æURL...")
        url_type = detector.detect_url_type(args.url)
        
        # è·å–æ¨èç­–ç•¥
        strategy = None
        html_content = None
        
        # å¦‚æœæ˜¯ç½‘é¡µï¼Œå…ˆè·å–HTMLå†…å®¹è¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ
        if url_type == 'webpage' and not args.force_hls and not args.force_capture:
            print("ğŸ“„ è·å–ç½‘é¡µå†…å®¹è¿›è¡Œåˆ†æ...")
            html_content = get_html_content(args.url, args.proxy, logger)
            if html_content:
                strategy = detector.recommend_strategy(args.url, html_content)
            else:
                print("âš ï¸  æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼Œå°†å°è¯•æŠ“åŒ…æ¨¡å¼")
                strategy = {'method': 'capture_and_analyze', 'use_capture': True, 'use_merge': True}
        else:
            strategy = detector.recommend_strategy(args.url)
        
        # åº”ç”¨æ‰‹åŠ¨è¦†ç›–
        if args.force_capture:
            strategy['use_capture'] = True
            strategy['method'] = 'capture_and_analyze'
            print("ğŸ”§ æ‰‹åŠ¨å¯ç”¨æŠ“åŒ…æ¨¡å¼")
        
        if args.force_hls:
            strategy['use_hls'] = True
            strategy['method'] = 'hls_download'
            print("ğŸ”§ æ‰‹åŠ¨å¯ç”¨HLSä¸‹è½½æ¨¡å¼")
        
        if args.no_merge:
            strategy['use_merge'] = False
            print("ğŸ”§ å·²ç¦ç”¨è‡ªåŠ¨éŸ³è§†é¢‘åˆå¹¶")
        else:
            strategy['use_merge'] = strategy.get('use_merge', False)
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œç­–ç•¥
        print(f"\nâœ… æ£€æµ‹å®Œæˆ")
        print(f"   URLç±»å‹: {url_type}")
        print(f"   æ¨èç­–ç•¥: {strategy['method']}")
        print()
        
        # æ ¹æ®ç­–ç•¥æ‰§è¡Œä¸‹è½½
        video_links = []
        captured_cookies = None
        captured_referer = None
        
        # ç­–ç•¥1: HLSæµä¸‹è½½
        if strategy['method'] == 'hls_download':
            print("ğŸ“º HLSæµä¸‹è½½æ¨¡å¼")
            print("-" * 70)
            stream_downloader = StreamDownloader(output_dir=args.output, logger=logger)
            output_file = stream_downloader.download_hls(args.url, "video.mp4")
            
            if output_file:
                print(f"\nâœ… ä¸‹è½½å®Œæˆ: {output_file}")
            else:
                print("\nâŒ ä¸‹è½½å¤±è´¥")
            return
        
        # ç­–ç•¥2: ç›´æ¥ä¸‹è½½è§†é¢‘æ–‡ä»¶
        elif strategy['method'] == 'direct_download':
            print("ğŸ“¥ ç›´æ¥ä¸‹è½½æ¨¡å¼")
            print("-" * 70)
            video_links = [args.url]
        
        # ç­–ç•¥3: æŠ“åŒ…åˆ†æ
        elif strategy['method'] == 'capture_and_analyze':
            if not optional_deps['selenium']:
                print("âš ï¸  æŠ“åŒ…æ¨¡å¼éœ€è¦seleniumï¼Œä½†æœªå®‰è£…")
                print("   å°è¯•ä½¿ç”¨HTMLè§£ææ¨¡å¼...")
                strategy['method'] = 'html_parse'
            else:
                print("ğŸŒ ç½‘ç»œæŠ“åŒ…æ¨¡å¼")
                print("-" * 70)
                print(f"   æ­£åœ¨å¯åŠ¨æµè§ˆå™¨å¹¶åˆ†æç½‘ç»œè¯·æ±‚...")
                print(f"   ç­‰å¾…æ—¶é—´: {args.wait_time}ç§’")
                print()
                
                capture = NetworkCapture(headless=True, logger=logger)
                requests_list = capture.start_capture(args.url, wait_time=args.wait_time)
                
                # è·å–Cookieå’ŒReferer
                captured_cookies = capture.get_cookies()
                captured_referer = capture.get_referer()
                
                if captured_cookies:
                    print(f"   âœ… è·å–åˆ° {len(captured_cookies)} ä¸ªCookie")
                if captured_referer:
                    print(f"   âœ… è·å–åˆ°Referer: {captured_referer[:50]}...")
                
                if requests_list:
                    # è§£æå…³é”®è¯
                    keywords = None
                    if args.keywords:
                        keywords = [k.strip() for k in args.keywords.split(',')]
                        print(f"   ğŸ” ä½¿ç”¨å…³é”®è¯æœç´¢: {', '.join(keywords)}")
                    
                    # è·å–æ‰€æœ‰è§†é¢‘å€™é€‰URLï¼ˆä½¿ç”¨å¢å¼ºçš„æœç´¢ï¼‰
                    print(f"\nğŸ“Š æ™ºèƒ½åˆ†æè§†é¢‘URL...")
                    candidates = capture.get_all_video_candidates(keywords=keywords)
                    
                    # æ˜¾ç¤ºåˆ†æç»“æœ
                    print(f"   é«˜ç½®ä¿¡åº¦: {len(candidates['high_confidence'])} ä¸ª")
                    print(f"   ä¸­ç­‰ç½®ä¿¡åº¦: {len(candidates['medium_confidence'])} ä¸ª")
                    if keywords:
                        print(f"   å…³é”®è¯åŒ¹é…: {len(candidates['keyword_matches'])} ä¸ª")
                    
                    # è¿‡æ»¤è§†é¢‘è¯·æ±‚
                    video_requests = capture.filter_video_requests(requests_list, keywords=keywords)
                    
                    if video_requests:
                        # ä¼˜å…ˆä½¿ç”¨å€™é€‰URL
                        print(f"\nğŸ¯ é€‰æ‹©æœ€ä½³è§†é¢‘URL...")
                        
                        # æ”¶é›†æ‰€æœ‰é«˜è´¨é‡çš„è§†é¢‘é“¾æ¥
                        priority_links = []
                        priority_links.extend(candidates['high_confidence'])
                        if keywords:
                            priority_links.extend(candidates['keyword_matches'])
                        priority_links.extend(candidates['medium_confidence'])
                        
                        # å»é‡
                        priority_links = list(dict.fromkeys(priority_links))
                        
                        if priority_links:
                            print(f"   æ‰¾åˆ° {len(priority_links)} ä¸ªä¼˜è´¨è§†é¢‘URL")
                            video_links.extend(priority_links[:args.max_downloads])
                        
                        # æå–æµåª’ä½“URL
                        streams = capture.extract_stream_urls(video_requests)
                        
                        print(f"\nğŸ“Š æµåª’ä½“åˆ†æ:")
                        print(f"   HLSæµ: {len(streams['hls'])} ä¸ª")
                        print(f"   DASHæµ: {len(streams['dash'])} ä¸ª")
                        print(f"   ç›´æ¥è§†é¢‘: {len(streams['direct'])} ä¸ª")
                        print(f"   è§†é¢‘ç‰‡æ®µ: {len(streams['segments'])} ä¸ª")
                        
                        # å¤„ç†HLSæµ
                        if streams['hls'] and not video_links:
                            print(f"\nğŸ¬ å‘ç°HLSæµï¼Œå¼€å§‹ä¸‹è½½...")
                            stream_downloader = StreamDownloader(output_dir=args.output, logger=logger)
                            output_file = stream_downloader.download_hls(streams['hls'][0], "hls_video.mp4")
                            if output_file:
                                print(f"âœ… HLSæµä¸‹è½½å®Œæˆ: {output_file}")
                        
                        # å¦‚æœè¿˜æ²¡æœ‰æ‰¾åˆ°è§†é¢‘é“¾æ¥ï¼Œä½¿ç”¨ç›´æ¥é“¾æ¥
                        if not video_links and streams['direct']:
                            video_links.extend(streams['direct'])
                        
                        # æ£€æµ‹åˆ†ç¦»çš„éŸ³è§†é¢‘æµ
                        if strategy.get('use_merge', False):
                            separate_result = detector.detect_separate_streams(video_requests)
                            
                            if separate_result['has_separate']:
                                print(f"\nğŸµ æ£€æµ‹åˆ°åˆ†ç¦»çš„éŸ³è§†é¢‘æµ")
                                print(f"   è§†é¢‘æµ: {len(separate_result['video_urls'])} ä¸ª")
                                print(f"   éŸ³é¢‘æµ: {len(separate_result['audio_urls'])} ä¸ª")
                                
                                # æ£€æŸ¥FFmpeg
                                merger = MediaMerger(logger=logger)
                                if merger.is_available():
                                    print(f"\nğŸ”§ å¼€å§‹ä¸‹è½½å¹¶åˆå¹¶éŸ³è§†é¢‘...")
                                    stream_downloader = StreamDownloader(output_dir=args.output, logger=logger)
                                    
                                    # é€‰æ‹©æœ€ä½³è´¨é‡çš„è§†é¢‘å’ŒéŸ³é¢‘
                                    video_url = separate_result['video_urls'][0]
                                    audio_url = separate_result['audio_urls'][0]
                                    
                                    output_file = stream_downloader.download_separate_streams(
                                        video_url, audio_url, "merged_video.mp4", merger
                                    )
                                    
                                    if output_file:
                                        print(f"âœ… éŸ³è§†é¢‘åˆå¹¶å®Œæˆ: {output_file}")
                                    else:
                                        print("âŒ éŸ³è§†é¢‘åˆå¹¶å¤±è´¥")
                                else:
                                    print("âš ï¸  æœªæ‰¾åˆ°FFmpegï¼Œæ— æ³•åˆå¹¶éŸ³è§†é¢‘")
                                    print("   æç¤º: å®‰è£…FFmpegä»¥å¯ç”¨éŸ³è§†é¢‘åˆå¹¶åŠŸèƒ½")
                    else:
                        print("âš ï¸  æœªæ‰¾åˆ°è§†é¢‘ç›¸å…³è¯·æ±‚ï¼Œå°è¯•HTMLè§£æ...")
                        strategy['method'] = 'html_parse'
                else:
                    print("âš ï¸  æœªæ•è·åˆ°ç½‘ç»œè¯·æ±‚ï¼Œå°è¯•HTMLè§£æ...")
                    strategy['method'] = 'html_parse'
        
        # ç­–ç•¥4: HTMLè§£æï¼ˆé»˜è®¤/å›é€€ï¼‰
        if strategy['method'] == 'html_parse':
            print("ğŸ“ HTMLè§£ææ¨¡å¼")
            print("-" * 70)
            
            if not html_content:
                html_content = get_html_content(args.url, args.proxy, logger)
            
            if html_content:
                parser = VideoParser(logger=logger)
                video_links = parser.parse(html_content, args.url)
            else:
                print("âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹")
                logger.error("æ— æ³•è·å–ç½‘é¡µå†…å®¹ï¼Œç¨‹åºé€€å‡º")
                sys.exit(1)
        
        # ä¸‹è½½è§†é¢‘é“¾æ¥
        if video_links:
            print(f"\nğŸ“¹ å…±æ‰¾åˆ° {len(video_links)} ä¸ªè§†é¢‘æ–‡ä»¶")
            
            # æ˜¾ç¤ºè§†é¢‘é“¾æ¥åˆ—è¡¨
            print("\nè§†é¢‘é“¾æ¥åˆ—è¡¨:")
            for i, link in enumerate(video_links[:args.max_downloads], 1):
                print(f"  {i}. {link}")
            
            # é™åˆ¶ä¸‹è½½æ•°é‡
            if len(video_links) > args.max_downloads:
                print(f"\nå°†ä¸‹è½½å‰ {args.max_downloads} ä¸ªè§†é¢‘æ–‡ä»¶")
                video_links = video_links[:args.max_downloads]
            
            # åˆ›å»ºä¸‹è½½å™¨ï¼ˆä½¿ç”¨æ•è·çš„Cookieå’ŒRefererï¼‰
            downloader = VideoDownloader(
                output_dir=args.output,
                workers=args.workers,
                retries=args.retries,
                proxy=args.proxy,
                resume=args.resume,
                verify=not args.no_verify,
                logger=logger,
                cookies=captured_cookies,
                referer=captured_referer
            )
            
            # å¼€å§‹ä¸‹è½½
            print(f"\nâ¬‡ï¸  å¼€å§‹ä¸‹è½½è§†é¢‘æ–‡ä»¶...")
            print()
            
            results = downloader.download_videos(video_links)
            
            # æ˜¾ç¤ºä¸‹è½½ç»“æœ
            print("\n" + "=" * 70)
            print("âœ… ä¸‹è½½å®Œæˆï¼")
            print("=" * 70)
            print(f"æˆåŠŸ: {results['success']} ä¸ª")
            print(f"å¤±è´¥: {results['failed']} ä¸ª")
            print(f"è·³è¿‡: {results['skipped']} ä¸ª")
            print(f"æ€»è®¡: {len(video_links)} ä¸ª")
            print("=" * 70)
            
            logger.info(f"ä¸‹è½½å®Œæˆ - æˆåŠŸ: {results['success']}, å¤±è´¥: {results['failed']}, è·³è¿‡: {results['skipped']}")
            
            # å¤„ç†åŠ å¯†è§†é¢‘æ–‡ä»¶
            if results['success'] > 0:
                print(f"\nğŸ”“ æ£€æŸ¥å¹¶å¤„ç†åŠ å¯†è§†é¢‘...")
                crypto_handler = EncryptedVideoHandler(logger=logger)
                processed = crypto_handler.batch_process_directory(args.output)
                
                if processed:
                    print(f"âœ… æˆåŠŸè§£å¯† {len(processed)} ä¸ªåŠ å¯†è§†é¢‘")
                    for file in processed:
                        print(f"   - {os.path.basename(file)}")
        else:
            print("\nâš ï¸  æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘")
        
        logger.info("ç¨‹åºç»“æŸ")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        logger.warning("ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(0)
    
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
